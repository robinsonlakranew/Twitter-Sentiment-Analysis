{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Twitter's API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# See https://developer.twitter.com/en/docs/basics/authentication/overview/oauth\n",
    "# for more information on Twitter's OAuth implementation.\n",
    "    \n",
    "\n",
    "import twitter\n",
    "\n",
    "def oauth_login():\n",
    "\n",
    "    CONSUMER_KEY = ''\n",
    "    CONSUMER_SECRET = ''\n",
    "    OAUTH_TOKEN = ''\n",
    "    OAUTH_TOKEN_SECRET = ''\n",
    "    \n",
    "    auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
    "                               CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    \n",
    "    twitter_api = twitter.Twitter(auth=auth)\n",
    "    return twitter_api\n",
    "\n",
    "# Sample usage\n",
    "twitter_api = oauth_login()    \n",
    "\n",
    "# Nothing to see by displaying twitter_api except that it's now a\n",
    "# defined variable\n",
    "\n",
    "print(twitter_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import flask\n",
    "print(flask.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discovering the trending topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import twitter\n",
    "\n",
    "def twitter_trends(twitter_api, woe_id):\n",
    "    # Prefix ID with the underscore for query string parameterization.\n",
    "    # Without the underscore, the twitter package appends the ID value\n",
    "    # to the URL itself as a special-case keyword argument.\n",
    "    return twitter_api.trends.place(_id=woe_id)\n",
    "\n",
    "# Sample usage\n",
    "\n",
    "twitter_api = oauth_login()\n",
    "\n",
    "# See https://developer.twitter.com/en/docs/trends/trends-for-location/api-reference/get-trends-place \n",
    "# and http://www.woeidlookup.com to look up different Yahoo! Where On Earth IDs\n",
    "\n",
    "WORLD_WOE_ID = 1\n",
    "world_trends = twitter_trends(twitter_api, WORLD_WOE_ID)\n",
    "print(json.dumps(world_trends, indent=1))\n",
    "\n",
    "US_WOE_ID = 23424977\n",
    "us_trends = twitter_trends(twitter_api, US_WOE_ID)\n",
    "print(json.dumps(us_trends, indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def twitter_search(twitter_api, q, max_results=200, **kw):\n",
    "\n",
    "    # See https://dev.twitter.com/docs/api/1.1/get/search/tweets    \n",
    "    search_results = twitter_api.search.tweets(q=q, count=100, **kw)\n",
    "    \n",
    "    statuses = search_results['statuses']\n",
    "    \n",
    "    # Iterate through batches of results by following the cursor until we\n",
    "    # reach the desired number of results, keeping in mind that OAuth users\n",
    "    # can \"only\" make 180 search queries per 15-minute interval. See\n",
    "    # https://developer.twitter.com/en/docs/basics/rate-limits\n",
    "    # for details. A reasonable number of results is ~1000, although\n",
    "    # that number of results may not exist for all queries.\n",
    "    \n",
    "    # Enforce a reasonable limit\n",
    "    max_results = min(1000, max_results)\n",
    "    \n",
    "    for _ in range(10): # 10*100 = 1000\n",
    "        try:\n",
    "            next_results = search_results['search_metadata']['next_results']\n",
    "        except KeyError as e: # No more results when next_results doesn't exist\n",
    "            break\n",
    "            \n",
    "        # Create a dictionary from next_results, which has the following form:\n",
    "        # ?max_id=313519052523986943&q=NCAA&include_entities=1\n",
    "        kwargs = dict([ kv.split('=') \n",
    "                        for kv in next_results[1:].split(\"&\") ])\n",
    "        \n",
    "        search_results = twitter_api.search.tweets(**kwargs)\n",
    "        statuses += search_results['statuses']\n",
    "        \n",
    "        if len(statuses) > max_results: \n",
    "            break\n",
    "            \n",
    "    return statuses\n",
    "\n",
    "# Sample usage\n",
    "\n",
    "twitter_api = oauth_login()\n",
    "\n",
    "q = \"CrossFit\"\n",
    "results = twitter_search(twitter_api, q, max_results=10)\n",
    "        \n",
    "# Show one sample search result by slicing the list...\n",
    "print(json.dumps(results[0], indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing convenient function calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "pp = partial(json.dumps, indent=1)\n",
    "\n",
    "twitter_world_trends = partial(twitter_trends, twitter_api, WORLD_WOE_ID)\n",
    "\n",
    "print(pp(twitter_world_trends()))\n",
    "\n",
    "authenticated_twitter_search = partial(twitter_search, twitter_api)\n",
    "results = authenticated_twitter_search(\"iPhone\")\n",
    "print(pp(results))\n",
    "\n",
    "authenticated_iphone_twitter_search = partial(authenticated_twitter_search, \"iPhone\")\n",
    "results = authenticated_iphone_twitter_search()\n",
    "print(pp(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and restoring JSON data with flat-text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io, json\n",
    "\n",
    "def save_json(filename, data):\n",
    "    with open('resources/ch09-twittercookbook/{0}.json'.format(filename),\n",
    "              'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False)\n",
    "\n",
    "\n",
    "def load_json(filename):\n",
    "    with open('resources/ch09-twittercookbook/{0}.json'.format(filename), \n",
    "              'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Sample usage\n",
    "\n",
    "q = 'CrossFit'\n",
    "\n",
    "twitter_api = oauth_login()\n",
    "results = twitter_search(twitter_api, q, max_results=10)\n",
    "\n",
    "save_json(q, results)\n",
    "results = load_json(q)\n",
    "\n",
    "print(json.dumps(results, indent=1, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and accessing JSON data with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "def save_to_pandas(data, fname):\n",
    "    df = pd.DataFrame.from_records(data)\n",
    "    df.to_pickle(fname)\n",
    "    return df\n",
    "    \n",
    "\n",
    "def load_from_mongo(fname):\n",
    "    df = pd.read_pickle(fname)\n",
    "    return df\n",
    "    \n",
    "\n",
    "# Sample usage\n",
    "\n",
    "q = 'CrossFit'\n",
    "\n",
    "twitter_api = oauth_login()\n",
    "results = twitter_search(twitter_api, q, max_results=10)\n",
    "\n",
    "df = save_to_pandas(results, 'search_results_{}.pkl'.format(q))\n",
    "\n",
    "df = load_from_mongo('search_results_{}.pkl'.format(q))\n",
    "\n",
    "# Show some sample output, but just the user and text columns\n",
    "df[['user','text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling the Twitter firehose with the Streaming API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finding topics of interest by using the filtering capabilities it offers.\n",
    "\n",
    "import sys\n",
    "import twitter\n",
    "\n",
    "# Query terms\n",
    "\n",
    "q = 'CrossFit' # Comma-separated list of terms\n",
    "\n",
    "print('Filtering the public timeline for track={0}'.format(q), file=sys.stderr)\n",
    "sys.stderr.flush()\n",
    "\n",
    "# Returns an instance of twitter.Twitter\n",
    "twitter_api = oauth_login()\n",
    "\n",
    "# Reference the self.auth parameter\n",
    "twitter_stream = twitter.TwitterStream(auth=twitter_api.auth)\n",
    "\n",
    "# See https://developer.twitter.com/en/docs/tutorials/consuming-streaming-data\n",
    "stream = twitter_stream.statuses.filter(track=q)\n",
    "\n",
    "# For illustrative purposes, when all else fails, search for Justin Bieber\n",
    "# and something is sure to turn up (at least, on Twitter)\n",
    "\n",
    "for tweet in stream:\n",
    "    print(tweet['text'])\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # Save to a database in a particular collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting time-series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "import twitter\n",
    "\n",
    "def get_time_series_data(api_func, mongo_db_name, mongo_db_coll, \n",
    "                         secs_per_interval=60, max_intervals=15, **mongo_conn_kw):\n",
    "    \n",
    "    # Default settings of 15 intervals and 1 API call per interval ensure that \n",
    "    # you will not exceed the Twitter rate limit.\n",
    "    \n",
    "    interval = 0\n",
    "    \n",
    "    while True:\n",
    "    \n",
    "        # A timestamp of the form \"2013-06-14 12:52:07\"\n",
    "        now = str(datetime.datetime.now()).split(\".\")[0]\n",
    "    \n",
    "        response = save_to_mongo(api_func(), mongo_db_name, \\\n",
    "                                 mongo_db_coll + \"-\" + now, **mongo_conn_kw)\n",
    "    \n",
    "        print(\"Write {0} trends\".format(len(response.inserted_ids)), file=sys.stderr)\n",
    "        print(\"Zzz...\", file=sys.stderr)\n",
    "        sys.stderr.flush()\n",
    "    \n",
    "        time.sleep(secs_per_interval) # seconds\n",
    "        interval += 1\n",
    "        \n",
    "        if interval >= 15:\n",
    "            break\n",
    "        \n",
    "# Sample usage\n",
    "\n",
    "get_time_series_data(twitter_world_trends, 'time-series', 'twitter_world_trends', host='mongodb://172.16.0.1:27017')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting tweet entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_tweet_entities(statuses):\n",
    "    \n",
    "    # See https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/entities-object\n",
    "    # for more details on tweet entities\n",
    "\n",
    "    if len(statuses) == 0:\n",
    "        return [], [], [], [], []\n",
    "    \n",
    "    screen_names = [ user_mention['screen_name'] \n",
    "                         for status in statuses\n",
    "                            for user_mention in status['entities']['user_mentions'] ]\n",
    "    \n",
    "    hashtags = [ hashtag['text'] \n",
    "                     for status in statuses \n",
    "                        for hashtag in status['entities']['hashtags'] ]\n",
    "\n",
    "    urls = [ url['expanded_url'] \n",
    "                     for status in statuses \n",
    "                        for url in status['entities']['urls'] ]\n",
    "               \n",
    "    # In some circumstances (such as search results), the media entity\n",
    "    # may not appear\n",
    "    medias = []\n",
    "    symbols = []\n",
    "    for status in statuses:\n",
    "        if 'media' in status['entities']:\n",
    "            for media in status['entities']['media']:\n",
    "                medias.append(media['url'])\n",
    "        if 'symbol' in status['entities']:\n",
    "            for symbol in status['entities']['symbol']:\n",
    "                symbols.append(symbol)\n",
    "    \n",
    "    return screen_names, hashtags, urls, medias, symbols\n",
    "\n",
    "# Sample usage\n",
    "\n",
    "q = 'CrossFit'\n",
    "\n",
    "statuses = twitter_search(twitter_api, q)\n",
    "\n",
    "screen_names, hashtags, urls, media, symbols = extract_tweet_entities(statuses)\n",
    "    \n",
    "# Explore the first five items for each...\n",
    "\n",
    "print(json.dumps(screen_names[0:5], indent=1))\n",
    "print(json.dumps(hashtags[0:5], indent=1))\n",
    "print(json.dumps(urls[0:5], indent=1))\n",
    "print(json.dumps(media[0:5], indent=1))\n",
    "print(json.dumps(symbols[0:5], indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the most popular tweets in a collection of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import twitter\n",
    "\n",
    "def find_popular_tweets(twitter_api, statuses, retweet_threshold=3):\n",
    "\n",
    "    # You could also consider using the favorite_count parameter as part of \n",
    "    # this  heuristic, possibly using it to provide an additional boost to \n",
    "    # popular tweets in a ranked formulation\n",
    "        \n",
    "    return [ status\n",
    "                for status in statuses \n",
    "                    if status['retweet_count'] > retweet_threshold ] \n",
    "    \n",
    "# Sample usage\n",
    "\n",
    "q = \"CrossFit\"\n",
    "\n",
    "twitter_api = oauth_login()\n",
    "search_results = twitter_search(twitter_api, q, max_results=200)\n",
    "\n",
    "popular_tweets = find_popular_tweets(twitter_api, search_results)\n",
    "\n",
    "for tweet in popular_tweets:\n",
    "    print(tweet['text'], tweet['retweet_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the most popular tweet entities in a collection of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import twitter\n",
    "from collections import Counter\n",
    "\n",
    "def get_common_tweet_entities(statuses, entity_threshold=3):\n",
    "\n",
    "    # Create a flat list of all tweet entities\n",
    "    tweet_entities = [  e\n",
    "                        for status in statuses\n",
    "                            for entity_type in extract_tweet_entities([status]) \n",
    "                                for e in entity_type \n",
    "                     ]\n",
    "\n",
    "    c = Counter(tweet_entities).most_common()\n",
    "\n",
    "    # Compute frequencies\n",
    "    return [ (k,v) \n",
    "             for (k,v) in c\n",
    "                 if v >= entity_threshold\n",
    "           ]\n",
    "\n",
    "# Sample usage\n",
    "\n",
    "q = 'CrossFit'\n",
    "\n",
    "twitter_api = oauth_login()\n",
    "search_results = twitter_search(twitter_api, q, max_results=100)\n",
    "common_entities = get_common_tweet_entities(search_results)\n",
    "\n",
    "print(\"Most common tweet entities\")\n",
    "print(common_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabulating frequency analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "# Get some frequency data\n",
    "\n",
    "twitter_api = oauth_login()\n",
    "search_results = twitter_search(twitter_api, q, max_results=100)\n",
    "common_entities = get_common_tweet_entities(search_results)\n",
    "\n",
    "# Use PrettyTable to create a nice tabular display\n",
    "\n",
    "pt = PrettyTable(field_names=['Entity', 'Count']) \n",
    "[ pt.add_row(kv) for kv in common_entities ]\n",
    "pt.align['Entity'], pt.align['Count'] = 'l', 'r' # Set column alignment\n",
    "pt._max_width = {'Entity':60, 'Count':10}\n",
    "print(pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding users who have retweeted a status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import twitter\n",
    "\n",
    "twitter_api = oauth_login()\n",
    "\n",
    "print(\"\"\"User IDs for retweeters of a tweet by @fperez_org\n",
    "that was retweeted by @SocialWebMining and that @jyeee then retweeted\n",
    "from @SocialWebMining's timeline\\n\"\"\")\n",
    "print(twitter_api.statuses.retweeters.ids(_id=334188056905129984)['ids'])\n",
    "print(json.dumps(twitter_api.statuses.show(_id=334188056905129984), indent=1))\n",
    "print()\n",
    "\n",
    "print(\"@SocialWeb's retweet of @fperez_org's tweet\\n\")\n",
    "print(twitter_api.statuses.retweeters.ids(_id=345723917798866944)['ids'])\n",
    "print(json.dumps(twitter_api.statuses.show(_id=345723917798866944), indent=1))\n",
    "print()\n",
    "\n",
    "print(\"@jyeee's retweet of @fperez_org's tweet\\n\")\n",
    "print(twitter_api.statuses.retweeters.ids(_id=338835939172417537)['ids'])\n",
    "print(json.dumps(twitter_api.statuses.show(_id=338835939172417537), indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting a retweet's attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_rt_attributions(tweet):\n",
    "\n",
    "    # Regex adapted from Stack Overflow (http://bit.ly/1821y0J)\n",
    "\n",
    "    rt_patterns = re.compile(r\"(RT|via)((?:\\b\\W*@\\w+)+)\", re.IGNORECASE)\n",
    "    rt_attributions = []\n",
    "\n",
    "    # Inspect the tweet to see if it was produced with /statuses/retweet/:id.\n",
    "    # See https://dev.twitter.com/docs/api/1.1/get/statuses/retweets/%3Aid.\n",
    "    \n",
    "    if 'retweeted_status' in tweet:\n",
    "        attribution = tweet['retweeted_status']['user']['screen_name'].lower()\n",
    "        rt_attributions.append(attribution)\n",
    "\n",
    "    # Also, inspect the tweet for the presence of \"legacy\" retweet patterns\n",
    "    # such as \"RT\" and \"via\", which are still widely used for various reasons\n",
    "    # and potentially very useful. See https://dev.twitter.com/discussions/2847 \n",
    "    # and https://dev.twitter.com/discussions/1748 for some details on how/why.\n",
    "\n",
    "    try:\n",
    "        rt_attributions += [ \n",
    "                        mention.strip() \n",
    "                        for mention in rt_patterns.findall(tweet['text'])[0][1].split() \n",
    "                      ]\n",
    "    except IndexError as e:\n",
    "        pass\n",
    "\n",
    "    # Filter out any duplicates\n",
    "\n",
    "    return list(set([rta.strip(\"@\").lower() for rta in rt_attributions]))\n",
    "\n",
    "# Sample usage\n",
    "twitter_api = oauth_login()\n",
    "\n",
    "tweet = twitter_api.statuses.show(_id=214746575765913602)\n",
    "print(get_rt_attributions(tweet))\n",
    "print()\n",
    "tweet = twitter_api.statuses.show(_id=345723917798866944)\n",
    "print(get_rt_attributions(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making robust Twitter requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from urllib.error import URLError\n",
    "from http.client import BadStatusLine\n",
    "import json\n",
    "import twitter\n",
    "\n",
    "def make_twitter_request(twitter_api_func, max_errors=10, *args, **kw): \n",
    "    \n",
    "    # A nested helper function that handles common HTTPErrors. Return an updated\n",
    "    # value for wait_period if the problem is a 500 level error. Block until the\n",
    "    # rate limit is reset if it's a rate limiting issue (429 error). Returns None\n",
    "    # for 401 and 404 errors, which requires special handling by the caller.\n",
    "    def handle_twitter_http_error(e, wait_period=2, sleep_when_rate_limited=True):\n",
    "    \n",
    "        if wait_period > 3600: # Seconds\n",
    "            print('Too many retries. Quitting.', file=sys.stderr)\n",
    "            raise e\n",
    "    \n",
    "        # See https://developer.twitter.com/en/docs/basics/response-codes\n",
    "        # for common codes\n",
    "    \n",
    "        if e.e.code == 401:\n",
    "            print('Encountered 401 Error (Not Authorized)', file=sys.stderr)\n",
    "            return None\n",
    "        elif e.e.code == 404:\n",
    "            print('Encountered 404 Error (Not Found)', file=sys.stderr)\n",
    "            return None\n",
    "        elif e.e.code == 429: \n",
    "            print('Encountered 429 Error (Rate Limit Exceeded)', file=sys.stderr)\n",
    "            if sleep_when_rate_limited:\n",
    "                print(\"Retrying in 15 minutes...ZzZ...\", file=sys.stderr)\n",
    "                sys.stderr.flush()\n",
    "                time.sleep(60*15 + 5)\n",
    "                print('...ZzZ...Awake now and trying again.', file=sys.stderr)\n",
    "                return 2\n",
    "            else:\n",
    "                raise e # Caller must handle the rate limiting issue\n",
    "        elif e.e.code in (500, 502, 503, 504):\n",
    "            print('Encountered {0} Error. Retrying in {1} seconds'\\\n",
    "                  .format(e.e.code, wait_period), file=sys.stderr)\n",
    "            time.sleep(wait_period)\n",
    "            wait_period *= 1.5\n",
    "            return wait_period\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "    # End of nested helper function\n",
    "    \n",
    "    wait_period = 2 \n",
    "    error_count = 0 \n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            return twitter_api_func(*args, **kw)\n",
    "        except twitter.api.TwitterHTTPError as e:\n",
    "            error_count = 0 \n",
    "            wait_period = handle_twitter_http_error(e, wait_period)\n",
    "            if wait_period is None:\n",
    "                return\n",
    "        except URLError as e:\n",
    "            error_count += 1\n",
    "            time.sleep(wait_period)\n",
    "            wait_period *= 1.5\n",
    "            print(\"URLError encountered. Continuing.\", file=sys.stderr)\n",
    "            if error_count > max_errors:\n",
    "                print(\"Too many consecutive errors...bailing out.\", file=sys.stderr)\n",
    "                raise\n",
    "        except BadStatusLine as e:\n",
    "            error_count += 1\n",
    "            time.sleep(wait_period)\n",
    "            wait_period *= 1.5\n",
    "            print(\"BadStatusLine encountered. Continuing.\", file=sys.stderr)\n",
    "            if error_count > max_errors:\n",
    "                print(\"Too many consecutive errors...bailing out.\", file=sys.stderr)\n",
    "                raise\n",
    "\n",
    "# Sample usage\n",
    "\n",
    "twitter_api = oauth_login()\n",
    "\n",
    "# See http://bit.ly/2Gcjfzr for twitter_api.users.lookup\n",
    "\n",
    "response = make_twitter_request(twitter_api.users.lookup, \n",
    "                                screen_name=\"SocialWebMining\")\n",
    "\n",
    "print(json.dumps(response, indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolving user profile information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_user_profile(twitter_api, screen_names=None, user_ids=None):\n",
    "   \n",
    "    # Must have either screen_name or user_id (logical xor)\n",
    "    assert (screen_names != None) != (user_ids != None), \\\n",
    "    \"Must have screen_names or user_ids, but not both\"\n",
    "    \n",
    "    items_to_info = {}\n",
    "\n",
    "    items = screen_names or user_ids\n",
    "    \n",
    "    while len(items) > 0:\n",
    "\n",
    "        # Process 100 items at a time per the API specifications for /users/lookup.\n",
    "        # See http://bit.ly/2Gcjfzr for details.\n",
    "        \n",
    "        items_str = ','.join([str(item) for item in items[:100]])\n",
    "        items = items[100:]\n",
    "\n",
    "        if screen_names:\n",
    "            response = make_twitter_request(twitter_api.users.lookup, \n",
    "                                            screen_name=items_str)\n",
    "        else: # user_ids\n",
    "            response = make_twitter_request(twitter_api.users.lookup, \n",
    "                                            user_id=items_str)\n",
    "    \n",
    "        for user_info in response:\n",
    "            if screen_names:\n",
    "                items_to_info[user_info['screen_name']] = user_info\n",
    "            else: # user_ids\n",
    "                items_to_info[user_info['id']] = user_info\n",
    "\n",
    "    return items_to_info\n",
    "\n",
    "# Sample usage\n",
    "\n",
    "twitter_api = oauth_login()\n",
    "\n",
    "print(get_user_profile(twitter_api, screen_names=[\"SocialWebMining\", \"ptwobrussell\"]))\n",
    "#print(get_user_profile(twitter_api, user_ids=[132373965]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting tweet entities from arbitrary text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pip install twitter_text\n",
    "import twitter_text\n",
    "\n",
    "# Sample usage\n",
    "\n",
    "txt = \"RT @SocialWebMining Mining 1M+ Tweets About #Syria http://wp.me/p3QiJd-1I\"\n",
    "\n",
    "ex = twitter_text.Extractor(txt)\n",
    "\n",
    "print(\"Screen Names:\", ex.extract_mentioned_screen_names_with_indices())\n",
    "print(\"URLs:\", ex.extract_urls_with_indices())\n",
    "print(\"Hashtags:\", ex.extract_hashtags_with_indices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting all friends or followers for a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from sys import maxsize as maxint\n",
    "\n",
    "def get_friends_followers_ids(twitter_api, screen_name=None, user_id=None,\n",
    "                              friends_limit=maxint, followers_limit=maxint):\n",
    "    \n",
    "    # Must have either screen_name or user_id (logical xor)\n",
    "    assert (screen_name != None) != (user_id != None), \\\n",
    "    \"Must have screen_name or user_id, but not both\"\n",
    "    \n",
    "    # See http://bit.ly/2GcjKJP and http://bit.ly/2rFz90N for details\n",
    "    # on API parameters\n",
    "    \n",
    "    get_friends_ids = partial(make_twitter_request, twitter_api.friends.ids, \n",
    "                              count=5000)\n",
    "    get_followers_ids = partial(make_twitter_request, twitter_api.followers.ids, \n",
    "                                count=5000)\n",
    "\n",
    "    friends_ids, followers_ids = [], []\n",
    "    \n",
    "    for twitter_api_func, limit, ids, label in [\n",
    "                    [get_friends_ids, friends_limit, friends_ids, \"friends\"], \n",
    "                    [get_followers_ids, followers_limit, followers_ids, \"followers\"]\n",
    "                ]:\n",
    "        \n",
    "        if limit == 0: continue\n",
    "        \n",
    "        cursor = -1\n",
    "        while cursor != 0:\n",
    "        \n",
    "            # Use make_twitter_request via the partially bound callable...\n",
    "            if screen_name: \n",
    "                response = twitter_api_func(screen_name=screen_name, cursor=cursor)\n",
    "            else: # user_id\n",
    "                response = twitter_api_func(user_id=user_id, cursor=cursor)\n",
    "\n",
    "            if response is not None:\n",
    "                ids += response['ids']\n",
    "                cursor = response['next_cursor']\n",
    "        \n",
    "            print('Fetched {0} total {1} ids for {2}'.format(len(ids),\\\n",
    "                  label, (user_id or screen_name)),file=sys.stderr)\n",
    "        \n",
    "            # XXX: You may want to store data during each iteration to provide an \n",
    "            # an additional layer of protection from exceptional circumstances\n",
    "        \n",
    "            if len(ids) >= limit or response is None:\n",
    "                break\n",
    "\n",
    "    # Do something useful with the IDs, like store them to disk...\n",
    "    return friends_ids[:friends_limit], followers_ids[:followers_limit]\n",
    "\n",
    "# Sample usage\n",
    "\n",
    "twitter_api = oauth_login()\n",
    "\n",
    "friends_ids, followers_ids = get_friends_followers_ids(twitter_api, \n",
    "                                                       screen_name=\"SocialWebMining\", \n",
    "                                                       friends_limit=10, \n",
    "                                                       followers_limit=10)\n",
    "\n",
    "print(friends_ids)\n",
    "print(followers_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing a user's friends and followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setwise_friends_followers_analysis(screen_name, friends_ids, followers_ids):\n",
    "    \n",
    "    friends_ids, followers_ids = set(friends_ids), set(followers_ids)\n",
    "    \n",
    "    print('{0} is following {1}'.format(screen_name, len(friends_ids)))\n",
    "\n",
    "    print('{0} is being followed by {1}'.format(screen_name, len(followers_ids)))\n",
    "    \n",
    "    print('{0} of {1} are not following {2} back'.format(\n",
    "            len(friends_ids.difference(followers_ids)), \n",
    "            len(friends_ids), screen_name))\n",
    "    \n",
    "    print('{0} of {1} are not being followed back by {2}'.format(\n",
    "            len(followers_ids.difference(friends_ids)), \n",
    "            len(followers_ids), screen_name))\n",
    "    \n",
    "    print('{0} has {1} mutual friends'.format(\n",
    "            screen_name, len(friends_ids.intersection(followers_ids))))\n",
    "    \n",
    "# Sample usage\n",
    "\n",
    "screen_name = \"ptwobrussell\"\n",
    "\n",
    "twitter_api = oauth_login()\n",
    "\n",
    "friends_ids, followers_ids = get_friends_followers_ids(twitter_api, \n",
    "                                                       screen_name=screen_name)\n",
    "setwise_friends_followers_analysis(screen_name, friends_ids, followers_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harvesting a user's tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def harvest_user_timeline(twitter_api, screen_name=None, user_id=None, max_results=1000):\n",
    "     \n",
    "    assert (screen_name != None) != (user_id != None), \\\n",
    "    \"Must have screen_name or user_id, but not both\"    \n",
    "    \n",
    "    kw = {  # Keyword args for the Twitter API call\n",
    "        'count': 200,\n",
    "        'trim_user': 'true',\n",
    "        'include_rts' : 'true',\n",
    "        'since_id' : 1\n",
    "        }\n",
    "    \n",
    "    if screen_name:\n",
    "        kw['screen_name'] = screen_name\n",
    "    else:\n",
    "        kw['user_id'] = user_id\n",
    "        \n",
    "    max_pages = 16\n",
    "    results = []\n",
    "    \n",
    "    tweets = make_twitter_request(twitter_api.statuses.user_timeline, **kw)\n",
    "    \n",
    "    if tweets is None: # 401 (Not Authorized) - Need to bail out on loop entry\n",
    "        tweets = []\n",
    "        \n",
    "    results += tweets\n",
    "    \n",
    "    print('Fetched {0} tweets'.format(len(tweets)), file=sys.stderr)\n",
    "    \n",
    "    page_num = 1\n",
    "    \n",
    "    # Many Twitter accounts have fewer than 200 tweets so you don't want to enter\n",
    "    # the loop and waste a precious request if max_results = 200.\n",
    "    \n",
    "    # Note: Analogous optimizations could be applied inside the loop to try and \n",
    "    # save requests. e.g. Don't make a third request if you have 287 tweets out of \n",
    "    # a possible 400 tweets after your second request. Twitter does do some \n",
    "    # post-filtering on censored and deleted tweets out of batches of 'count', though,\n",
    "    # so you can't strictly check for the number of results being 200. You might get\n",
    "    # back 198, for example, and still have many more tweets to go. If you have the\n",
    "    # total number of tweets for an account (by GET /users/lookup/), then you could \n",
    "    # simply use this value as a guide.\n",
    "    \n",
    "    if max_results == kw['count']:\n",
    "        page_num = max_pages # Prevent loop entry\n",
    "    \n",
    "    while page_num < max_pages and len(tweets) > 0 and len(results) < max_results:\n",
    "    \n",
    "        # Necessary for traversing the timeline in Twitter's v1.1 API:\n",
    "        # get the next query's max-id parameter to pass in.\n",
    "        # See https://dev.twitter.com/docs/working-with-timelines.\n",
    "        kw['max_id'] = min([ tweet['id'] for tweet in tweets]) - 1 \n",
    "    \n",
    "        tweets = make_twitter_request(twitter_api.statuses.user_timeline, **kw)\n",
    "        results += tweets\n",
    "\n",
    "        print('Fetched {0} tweets'.format(len(tweets)),file=sys.stderr)\n",
    "    \n",
    "        page_num += 1\n",
    "        \n",
    "    print('Done fetching tweets', file=sys.stderr)\n",
    "\n",
    "    return results[:max_results]\n",
    "    \n",
    "# Sample usage\n",
    "\n",
    "twitter_api = oauth_login()\n",
    "tweets = harvest_user_timeline(twitter_api, screen_name=\"SocialWebMining\", \\\n",
    "                               max_results=200)\n",
    "\n",
    "# Save to MongoDB with save_to_mongo or a local file with save_json..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling a friendship graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crawl_followers(twitter_api, screen_name, limit=1000000, depth=2, **mongo_conn_kw):\n",
    "    \n",
    "    # Resolve the ID for screen_name and start working with IDs for consistency \n",
    "    # in storage\n",
    "\n",
    "    seed_id = str(twitter_api.users.show(screen_name=screen_name)['id'])\n",
    "    \n",
    "    _, next_queue = get_friends_followers_ids(twitter_api, user_id=seed_id, \n",
    "                                              friends_limit=0, followers_limit=limit)\n",
    "\n",
    "    # Store a seed_id => _follower_ids mapping in MongoDB\n",
    "    \n",
    "    save_to_mongo({'followers' : [ _id for _id in next_queue ]}, 'followers_crawl', \n",
    "                  '{0}-follower_ids'.format(seed_id), **mongo_conn_kw)\n",
    "    \n",
    "    d = 1\n",
    "    while d < depth:\n",
    "        d += 1\n",
    "        (queue, next_queue) = (next_queue, [])\n",
    "        for fid in queue:\n",
    "            _, follower_ids = get_friends_followers_ids(twitter_api, user_id=fid, \n",
    "                                                     friends_limit=0, \n",
    "                                                     followers_limit=limit)\n",
    "            \n",
    "            # Store a fid => follower_ids mapping in MongoDB\n",
    "            save_to_mongo({'followers' : [ _id for _id in follower_ids ]}, \n",
    "                          'followers_crawl', '{0}-follower_ids'.format(fid))\n",
    "            \n",
    "            next_queue += follower_ids\n",
    "\n",
    "# Sample usage\n",
    "\n",
    "screen_name = \"timoreilly\"\n",
    "\n",
    "twitter_api = oauth_login()\n",
    "\n",
    "crawl_followers(twitter_api, screen_name, depth=1, limit=10, host='mongodb://172.16.0.1:27017')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing tweet content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyze_tweet_content(statuses):\n",
    "    \n",
    "    if len(statuses) == 0:\n",
    "        print(\"No statuses to analyze\")\n",
    "        return\n",
    "    \n",
    "    # A nested helper function for computing lexical diversity\n",
    "    def lexical_diversity(tokens):\n",
    "        return 1.0*len(set(tokens))/len(tokens) \n",
    "    \n",
    "    # A nested helper function for computing the average number of words per tweet\n",
    "    def average_words(statuses):\n",
    "        total_words = sum([ len(s.split()) for s in statuses ]) \n",
    "        return 1.0*total_words/len(statuses)\n",
    "\n",
    "    status_texts = [ status['text'] for status in statuses ]\n",
    "    screen_names, hashtags, urls, media, _ = extract_tweet_entities(statuses)\n",
    "    \n",
    "    # Compute a collection of all words from all tweets\n",
    "    words = [ w \n",
    "          for t in status_texts \n",
    "              for w in t.split() ]\n",
    "    \n",
    "    print(\"Lexical diversity (words):\", lexical_diversity(words))\n",
    "    print(\"Lexical diversity (screen names):\", lexical_diversity(screen_names))\n",
    "    print(\"Lexical diversity (hashtags):\", lexical_diversity(hashtags))\n",
    "    print(\"Averge words per tweet:\", average_words(status_texts))\n",
    "\n",
    "    \n",
    "# Sample usage\n",
    "\n",
    "q = 'CrossFit'\n",
    "twitter_api = oauth_login()\n",
    "search_results = twitter_search(twitter_api, q)\n",
    "\n",
    "analyze_tweet_content(search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing link targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import nltk\n",
    "import numpy\n",
    "import requests\n",
    "from boilerpipe.extract import Extractor\n",
    "\n",
    "def summarize(url=None, html=None, n=100, cluster_threshold=5, top_sentences=5):\n",
    "\n",
    "    # Adapted from \"The Automatic Creation of Literature Abstracts\" by H.P. Luhn\n",
    "    #\n",
    "    # Parameters:\n",
    "    # * n  - Number of words to consider\n",
    "    # * cluster_threshold - Distance between words to consider\n",
    "    # * top_sentences - Number of sentences to return for a \"top n\" summary\n",
    "            \n",
    "    # Begin - nested helper function\n",
    "    def score_sentences(sentences, important_words):\n",
    "        scores = []\n",
    "        sentence_idx = -1\n",
    "    \n",
    "        for s in [nltk.tokenize.word_tokenize(s) for s in sentences]:\n",
    "    \n",
    "            sentence_idx += 1\n",
    "            word_idx = []\n",
    "    \n",
    "            # For each word in the word list...\n",
    "            for w in important_words:\n",
    "                try:\n",
    "                    # Compute an index for important words in each sentence\n",
    "    \n",
    "                    word_idx.append(s.index(w))\n",
    "                except ValueError as e: # w not in this particular sentence\n",
    "                    pass\n",
    "    \n",
    "            word_idx.sort()\n",
    "    \n",
    "            # It is possible that some sentences may not contain any important words\n",
    "            if len(word_idx)== 0: continue\n",
    "    \n",
    "            # Using the word index, compute clusters with a max distance threshold\n",
    "            # for any two consecutive words\n",
    "    \n",
    "            clusters = []\n",
    "            cluster = [word_idx[0]]\n",
    "            i = 1\n",
    "            while i < len(word_idx):\n",
    "                if word_idx[i] - word_idx[i - 1] < cluster_threshold:\n",
    "                    cluster.append(word_idx[i])\n",
    "                else:\n",
    "                    clusters.append(cluster[:])\n",
    "                    cluster = [word_idx[i]]\n",
    "                i += 1\n",
    "            clusters.append(cluster)\n",
    "    \n",
    "            # Score each cluster. The max score for any given cluster is the score \n",
    "            # for the sentence.\n",
    "    \n",
    "            max_cluster_score = 0\n",
    "            for c in clusters:\n",
    "                significant_words_in_cluster = len(c)\n",
    "                total_words_in_cluster = c[-1] - c[0] + 1\n",
    "                score = 1.0 * significant_words_in_cluster \\\n",
    "                    * significant_words_in_cluster / total_words_in_cluster\n",
    "    \n",
    "                if score > max_cluster_score:\n",
    "                    max_cluster_score = score\n",
    "    \n",
    "            scores.append((sentence_idx, score))\n",
    "    \n",
    "        return scores    \n",
    "    \n",
    "    # End - nested helper function\n",
    "    \n",
    "    extractor = Extractor(extractor='ArticleExtractor', url=url, html=html)\n",
    "\n",
    "    # It's entirely possible that this \"clean page\" will be a big mess. YMMV.\n",
    "    # The good news is that the summarize algorithm inherently accounts for handling\n",
    "    # a lot of this noise.\n",
    "\n",
    "    txt = extractor.getText()\n",
    "    \n",
    "    sentences = [s for s in nltk.tokenize.sent_tokenize(txt)]\n",
    "    normalized_sentences = [s.lower() for s in sentences]\n",
    "\n",
    "    words = [w.lower() for sentence in normalized_sentences for w in\n",
    "             nltk.tokenize.word_tokenize(sentence)]\n",
    "\n",
    "    fdist = nltk.FreqDist(words)\n",
    "\n",
    "    top_n_words = [w[0] for w in fdist.items() \n",
    "            if w[0] not in nltk.corpus.stopwords.words('english')][:n]\n",
    "\n",
    "    scored_sentences = score_sentences(normalized_sentences, top_n_words)\n",
    "\n",
    "    # Summarization Approach 1:\n",
    "    # Filter out nonsignificant sentences by using the average score plus a\n",
    "    # fraction of the std dev as a filter\n",
    "\n",
    "    avg = numpy.mean([s[1] for s in scored_sentences])\n",
    "    std = numpy.std([s[1] for s in scored_sentences])\n",
    "    mean_scored = [(sent_idx, score) for (sent_idx, score) in scored_sentences\n",
    "                   if score > avg + 0.5 * std]\n",
    "\n",
    "    # Summarization Approach 2:\n",
    "    # Another approach would be to return only the top N ranked sentences\n",
    "\n",
    "    top_n_scored = sorted(scored_sentences, key=lambda s: s[1])[-top_sentences:]\n",
    "    top_n_scored = sorted(top_n_scored, key=lambda s: s[0])\n",
    "\n",
    "    # Decorate the post object with summaries\n",
    "\n",
    "    return dict(top_n_summary=[sentences[idx] for (idx, score) in top_n_scored],\n",
    "                mean_scored_summary=[sentences[idx] for (idx, score) in mean_scored])\n",
    "\n",
    "# Sample usage\n",
    "\n",
    "sample_url = 'http://radar.oreilly.com/2013/06/phishing-in-facebooks-pond.html'\n",
    "summary = summarize(url=sample_url)\n",
    "\n",
    "\n",
    "# sample_html = requests.get(sample_url).text\n",
    "# summary = summarize(html=sample_html)\n",
    "\n",
    "print(\"-------------------------------------------------\")\n",
    "print(\"                'Top N Summary'\")\n",
    "print(\"-------------------------------------------------\")\n",
    "print(\" \".join(summary['top_n_summary']))\n",
    "print()\n",
    "print()\n",
    "print(\"-------------------------------------------------\")\n",
    "print(\"             'Mean Scored' Summary\")\n",
    "print(\"-------------------------------------------------\")\n",
    "print(\" \".join(summary['mean_scored_summary']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing a user's favorite tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyze_favorites(twitter_api, screen_name, entity_threshold=2):\n",
    "    \n",
    "    # Could fetch more than 200 by walking the cursor as shown in other\n",
    "    # recipes, but 200 is a good sample to work with.\n",
    "    favs = twitter_api.favorites.list(screen_name=screen_name, count=200)\n",
    "    print(\"Number of favorites:\", len(favs))\n",
    "    \n",
    "    # Figure out what some of the common entities are, if any, in the content\n",
    "    \n",
    "    common_entities = get_common_tweet_entities(favs, \n",
    "                                                entity_threshold=entity_threshold)\n",
    "    \n",
    "    # Use PrettyTable to create a nice tabular display\n",
    "    \n",
    "    pt = PrettyTable(field_names=['Entity', 'Count']) \n",
    "    [ pt.add_row(kv) for kv in common_entities ]\n",
    "    pt.align['Entity'], pt.align['Count'] = 'l', 'r' # Set column alignment\n",
    "    \n",
    "    print()\n",
    "    print(\"Common entities in favorites...\")\n",
    "    print(pt)\n",
    "    \n",
    "    \n",
    "    # Print out some other stats\n",
    "    print()\n",
    "    print(\"Some statistics about the content of the favorities...\")\n",
    "    print()\n",
    "    analyze_tweet_content(favs)\n",
    "    \n",
    "    # Could also start analyzing link content or summarized link content, and more.\n",
    "\n",
    "# Sample usage\n",
    "\n",
    "twitter_api = oauth_login()\n",
    "analyze_favorites(twitter_api, \"ptwobrussell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Yahoo! Where On Earth ID for the entire world is 1.\n",
    "# See https://dev.twitter.com/docs/api/1.1/get/trends/place and\n",
    "# http://developer.yahoo.com/geo/geoplanet/\n",
    "\n",
    "WORLD_WOE_ID = 1\n",
    "US_WOE_ID = 23424977\n",
    "\n",
    "# Prefix ID with the underscore for query string parameterization.\n",
    "# Without the underscore, the twitter package appends the ID value\n",
    "# to the URL itself as a special case keyword argument.\n",
    "\n",
    "world_trends = twitter_api.trends.place(_id=WORLD_WOE_ID)\n",
    "us_trends = twitter_api.trends.place(_id=US_WOE_ID)\n",
    "\n",
    "print(world_trends)\n",
    "print()\n",
    "print(us_trends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trend in us_trends[0]['trends']:\n",
    "    print(trend['name'])\n",
    "    \n",
    "world_trends_set = set([trend['name'] \n",
    "                        for trend in world_trends[0]['trends']])\n",
    "\n",
    "us_trends_set = set([trend['name'] \n",
    "                     for trend in us_trends[0]['trends']]) \n",
    "\n",
    "common_trends = world_trends_set.intersection(us_trends_set)\n",
    "\n",
    "print(common_trends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anatomy of a Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "q = '#Walmart' \n",
    "\n",
    "count = 100\n",
    "\n",
    "# Import unquote to prevent url encoding errors in next_results\n",
    "from urllib.parse import unquote\n",
    "\n",
    "# See https://dev.twitter.com/rest/reference/get/search/tweets\n",
    "\n",
    "search_results = twitter_api.search.tweets(q=q, count=count)\n",
    "\n",
    "statuses = search_results['statuses']\n",
    "\n",
    "\n",
    "# Iterate through 5 more batches of results by following the cursor\n",
    "for _ in range(5):\n",
    "    print('Length of statuses', len(statuses))\n",
    "    try:\n",
    "        next_results = search_results['search_metadata']['next_results']\n",
    "    except KeyError as e: # No more results when next_results doesn't exist\n",
    "        break\n",
    "        \n",
    "    # Create a dictionary from next_results, which has the following form:\n",
    "    # ?max_id=847960489447628799&q=%23RIPSelena&count=100&include_entities=1\n",
    "    kwargs = dict([ kv.split('=') for kv in unquote(next_results[1:]).split(\"&\") ])\n",
    "    \n",
    "    search_results = twitter_api.search.tweets(**kwargs)\n",
    "    statuses += search_results['statuses']\n",
    "\n",
    "# Show one sample search result by slicing the list...\n",
    "print(json.dumps(statuses[0], indent=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print()\n",
    "    print(statuses[i]['text'])\n",
    "    print('Favorites: ', statuses[i]['favorite_count'])\n",
    "    print('Retweets: ', statuses[i]['retweet_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting text, screen names, and hashtags from tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_texts = [ status['text'] \n",
    "                 for status in statuses ]\n",
    "\n",
    "screen_names = [ user_mention['screen_name'] \n",
    "                 for status in statuses\n",
    "                     for user_mention in status['entities']['user_mentions'] ]\n",
    "\n",
    "hashtags = [ hashtag['text'] \n",
    "             for status in statuses\n",
    "                 for hashtag in status['entities']['hashtags'] ]\n",
    "\n",
    "# Compute a collection of all words from all tweets\n",
    "words = [ w \n",
    "          for t in status_texts \n",
    "              for w in t.split() ]\n",
    "\n",
    "# Explore the first 5 items for each...\n",
    "\n",
    "print(json.dumps(status_texts[0:5], indent=1))\n",
    "print(json.dumps(screen_names[0:5], indent=1) )\n",
    "print(json.dumps(hashtags[0:5], indent=1))\n",
    "print(json.dumps(words[0:5], indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a basic frequency distribution from the words in tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for item in [words, screen_names, hashtags]:\n",
    "    c = Counter(item)\n",
    "    print(c.most_common()[:10]) # top 10\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using prettytable to display tuples in a nice tabular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "for label, data in (('Word', words), \n",
    "                    ('Screen Name', screen_names), \n",
    "                    ('Hashtag', hashtags)):\n",
    "    pt = PrettyTable(field_names=[label, 'Count']) \n",
    "    c = Counter(data)\n",
    "    [ pt.add_row(kv) for kv in c.most_common()[:10] ]\n",
    "    pt.align[label], pt.align['Count'] = 'l', 'r' # Set column alignment\n",
    "    print(pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating lexical diversity for tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for computing lexical diversity\n",
    "def lexical_diversity(tokens):\n",
    "    return len(set(tokens))/len(tokens) \n",
    "\n",
    "# A function for computing the average number of words per tweet\n",
    "def average_words(statuses):\n",
    "    total_words = sum([ len(s.split()) for s in statuses ]) \n",
    "    return total_words/len(statuses)\n",
    "\n",
    "print(lexical_diversity(words))\n",
    "print(lexical_diversity(screen_names))\n",
    "print(lexical_diversity(hashtags))\n",
    "print(average_words(status_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the most popular retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets = [\n",
    "            # Store out a tuple of these three values ...\n",
    "            (status['retweet_count'], \n",
    "             status['retweeted_status']['user']['screen_name'],\n",
    "             status['retweeted_status']['id'],\n",
    "             status['text']) \n",
    "            \n",
    "            # ... for each status ...\n",
    "            for status in statuses \n",
    "            \n",
    "            # ... so long as the status meets this condition.\n",
    "                if 'retweeted_status' in status.keys()\n",
    "           ]\n",
    "\n",
    "# Slice off the first 5 from the sorted results and display each item in the tuple\n",
    "\n",
    "pt = PrettyTable(field_names=['Count', 'Screen Name', 'Tweet ID', 'Text'])\n",
    "[ pt.add_row(row) for row in sorted(retweets, reverse=True)[:5] ]\n",
    "pt.max_width['Text'] = 50\n",
    "pt.align= 'l'\n",
    "print(pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking up users who have retweeted a status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the original tweet id for a tweet from its retweeted_status node \n",
    "# and insert it here\n",
    "\n",
    "_retweets = twitter_api.statuses.retweets(id=862359093398261760)\n",
    "print([r['user']['screen_name'] for r in _retweets])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting frequencies of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "word_counts = sorted(Counter(words).values(), reverse=True)\n",
    "\n",
    "plt.loglog(word_counts)\n",
    "plt.ylabel(\"Freq\")\n",
    "plt.xlabel(\"Word Rank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating histograms of words, screen names, and hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, data in (('Words', words), \n",
    "                    ('Screen Names', screen_names), \n",
    "                    ('Hashtags', hashtags)):\n",
    "\n",
    "    # Build a frequency map for each set of data\n",
    "    # and plot the values\n",
    "    c = Counter(data)\n",
    "    plt.hist(list(c.values()))\n",
    "    \n",
    "    # Add a title and y-label ...\n",
    "    plt.title(label)\n",
    "    plt.ylabel(\"Number of items in bin\")\n",
    "    plt.xlabel(\"Bins (number of times an item appeared)\")\n",
    "    \n",
    "    # ... and display as a new figure\n",
    "    plt.figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a histogram of retweet counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using underscores while unpacking values in\n",
    "# a tuple is idiomatic for discarding them\n",
    "\n",
    "counts = [count for count, _, _, _ in retweets]\n",
    "\n",
    "plt.hist(counts)\n",
    "plt.title('Retweets')\n",
    "plt.xlabel('Bins (number of times retweeted)')\n",
    "plt.ylabel('Number of tweets in bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "twitter_stream = twitter.TwitterStream(auth=auth)\n",
    "iterator = twitter_stream.statuses.sample()\n",
    "\n",
    "tweets = []\n",
    "for tweet in iterator:\n",
    "    try:\n",
    "        if tweet['lang'] == 'en':\n",
    "            tweets.append(tweet)\n",
    "    except:\n",
    "        pass\n",
    "    if len(tweets) == 100:\n",
    "        break\n",
    "        \n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "scores = np.zeros(len(tweets))\n",
    "\n",
    "for i, t in enumerate(tweets):\n",
    "    # Extract the text portion of the tweet\n",
    "    text = t['text']\n",
    "    \n",
    "    # Measure the polarity of the tweet\n",
    "    polarity = analyzer.polarity_scores(text)\n",
    "    \n",
    "    # Store the normalized, weighted composite score\n",
    "    scores[i] = polarity['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_positive = np.argmax(scores)\n",
    "most_negative = np.argmin(scores)\n",
    "\n",
    "print('{0:6.3f} : \"{1}\"'.format(scores[most_positive], tweets[most_positive]['text']))\n",
    "print('{0:6.3f} : \"{1}\"'.format(scores[most_negative], tweets[most_negative]['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis using sklearn naive_bayes(MultinomialNB, BernoulliNB),  RandomForestClassifier, SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import _pickle as cPickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataFrame(datafile):\n",
    "    print (\"reading data file...\")\n",
    "\n",
    "    data_df =  pd.read_csv(datafile, header=0, delimiter=\"\\t\", quoting=3)\n",
    "    print (\"data shape: \", data_df.shape)\n",
    "    print (\"data colums: \", data_df.columns.values)\n",
    "    return data_df\n",
    "\n",
    "def cleanDataFrame(dataframe):\n",
    "    print (\"pre-processing data...\")\n",
    "\n",
    "    for index, row in dataframe.iterrows():\n",
    "        row['message'] = messageToWords(row['message'])\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "original_training_df = getDataFrame(tweets)\n",
    "clean_training_df = cleanDataFrame(original_training_df.copy())\n",
    "\n",
    "training_set, validation_set = train_test_split(clean_training_df, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('vect', CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None)),\n",
    "                           ('tfidf', TfidfTransformer()),\n",
    "                           ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42))])\n",
    "\n",
    "pipelineNB = Pipeline([\n",
    "('count_vectorizer',   CountVectorizer(ngram_range=(1, 2))),\n",
    "('tfidf', TfidfTransformer()),\n",
    "('classifier',         BernoulliNB(binarize=0.0)) ])\n",
    "\n",
    "pipelineRFC = Pipeline([\n",
    "('count_vectorizer',   CountVectorizer(ngram_range=(1, 2))),\n",
    "('tfidf', TfidfTransformer()),\n",
    "('classifier', RandomForestClassifier(n_estimators = 100))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainModel(training_set,pipeline)\n",
    "predicted = predictModel(validation_set, model)\n",
    "target_domains = list(set(validation_set[\"domain\"]))\n",
    "print(classification_report(validation_set[\"dsomain\"], predicted, target_names=target_domains))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet expression analysis using Google News Data Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models as gsm\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from scipy import spatial\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download executable file using \n",
    "# \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\" \n",
    "\n",
    "# Extract than rename to trained_model.bin\n",
    "\n",
    "w2v = gsm.KeyedVectors.load_word2vec_format('trained_model.bin', binary=True, limit=50000) \n",
    "\n",
    "# Add words that proves text with strong sentiments\n",
    "\n",
    "SIMILAR_EMOTIONS = ['urgent', 'greed', 'panic', 'fear', 'worry', 'protect']\n",
    "\n",
    "CLONE_N = 30 \n",
    "VALID_POS_LIST = ['NN', 'VB', 'JJ']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(caption): \n",
    "    result = []\n",
    "    for sent in sent_tokenize(caption):\n",
    "        for word, pos in pos_tag(word_tokenize(sent)):\n",
    "            s = word.translate(string.punctuation)\n",
    "            for valid_pos in VALID_POS_LIST:\n",
    "                if valid_pos in pos:\n",
    "                    if s not in stop_words: \n",
    "                        try: \n",
    "                            vec = np.zeros_like(w2v['hello'])\n",
    "                            result.append(np.add(vec, w2v[s])) \n",
    "                        except: \n",
    "                            pass\n",
    "                    break\n",
    "    return result\n",
    "\n",
    "def distance_transformer(captions, emotions_list):\n",
    "    result = []\n",
    "    for caption in captions:\n",
    "        average_vec = np.mean(vectorize(caption), axis=0)\n",
    "        to_append = []\n",
    "        for emotion in emotions_list: \n",
    "            if emotions_list == SIMILAR_EMOTIONS:\n",
    "                dist = spatial.distance.cosine(average_vec, w2v[emotion]) \n",
    "#                 print(1-dist,emotion)\n",
    "            if math.isnan(dist):\n",
    "                # to_append.append([emotion,0.0*100])\n",
    "                to_append.append(0.0*100)\n",
    "            else: \n",
    "                # to_append.append([emotion,(1-dist)*100])\n",
    "                to_append.append((1-dist)*100)\n",
    "        result.append(to_append)\n",
    "    return to_append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If similarity score is higher than 30 \n",
    "\n",
    "def emotions(text):\n",
    "    d = distance_transformer([text], SIMILAR_EMOTIONS)\n",
    "    if max(d)>30:\n",
    "        return False\n",
    "    above_15 = [s for s in d if s>20]\n",
    "    if len(above_15)>=3:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "for tweet in tweets:\n",
    "    print('Strong sentiment') if emotions(tweet) else continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
